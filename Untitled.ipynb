{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "plrKUQ0E3g_U",
    "outputId": "694bc8ba-cf34-46b0-d3d1-c27f134c74e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sDqo5Vok18IZ"
   },
   "outputs": [],
   "source": [
    "# reading the data\n",
    "with open(\"drive/My Drive/RC_2010-01\", \"r\") as reddit:\n",
    "    reddit = reddit.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ns30d_mm4Li2"
   },
   "outputs": [],
   "source": [
    "# stemming and lemmatizing\n",
    "def normalize(dataset):\n",
    "    docs=dataset\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.stem import PorterStemmer\n",
    "    ps=PorterStemmer()\n",
    "    sen=[]\n",
    "    for i in docs:\n",
    "        sentence=i.split()\n",
    "        l=[]\n",
    "        for k in sentence:\n",
    "            h=ps.stem(k)\n",
    "\n",
    "            l.append(h)\n",
    "        p=\" \".join(l)   \n",
    "        sen.append(p)\n",
    "    wl= WordNetLemmatizer()\n",
    "    train_normalized=[]\n",
    "    for sentence_words in sen:\n",
    "        g=[]\n",
    "        for word in sentence_words.split():\n",
    "            h=wl.lemmatize(word, pos=\"v\")\n",
    "            g.append(h)\n",
    "        s=\" \".join(g)\n",
    "        train_normalized.append(s)\n",
    "    return train_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i3EP3Y1LCC8C"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = list()\n",
    "for line in reddit:\n",
    "    line = json.loads(line)\n",
    "    data.append(line[\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s1D7QEPNCDIQ"
   },
   "outputs": [],
   "source": [
    "def del_punc(comments):\n",
    "    import re\n",
    "    dataset = list()\n",
    "    for sentence in comments:\n",
    "        sentence = re.sub('[^a-zA-Z\\.\\?\\;\\.\\.+\\!\\!+]', ' ', sentence)\n",
    "        sentence = re.sub('\\.\\.+', '.', sentence)\n",
    "        sentence = re.sub('\\. ', '. ', sentence)\n",
    "        sentence = re.sub('\\?', '. ', sentence)\n",
    "        sentence = re.sub('\\!', '. ', sentence)\n",
    "        sentence = re.sub('\\!\\!+', '. ', sentence)\n",
    "        sentence = re.sub('\\?\\?+', '. ', sentence)\n",
    "        sentence = re.sub('\\;\\;+', '. ', sentence)\n",
    "        sentence = re.sub('\\:\\:+', ' ', sentence)\n",
    "        sentence = re.sub('[\\']', ' ', sentence)\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "        \n",
    "        dataset.append(sentence)\n",
    "    return dataset\n",
    "comments = del_punc(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p5rsGxPxbALy"
   },
   "source": [
    "# What is NLTK?\n",
    "NLTK stands for Natural Language Toolkit. This toolkit is one of the most powerful NLP libraries which contains packages to make machines understand human language and reply to it with an appropriate response.\n",
    "Tokenization, Stemming, Lemmatization, Punctuation, Character count, word count are some of these packages.\n",
    "With Using the library nltk, we downloading List of stopwords and Markings And we download network words.\n",
    "The project uses the nltk library It is received with the following lines of this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "4hpAYCihCDTi",
    "outputId": "eedcb7fa-82ee-4fc3-ae82-eb5d0ee74303"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# normalization is stemming and lemmatizing\n",
    "dataset = normalize(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gs1OSLvxCf_T"
   },
   "source": [
    "for labeling the comments i use SentimentIntensityAnalysis library from nltk.\n",
    "this library computes Intensity of each label(neg, neu, pos) for each comment. this Intensity is based on the words that exist in each comment. for labeling i use the maximum of the Intesities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qD9flqavLAMG"
   },
   "source": [
    "#Dataset will have 3 sentiments:\n",
    "\n",
    "1. positive : if score > 0\n",
    "2. negative: if score < 0\n",
    "3. zero : if score = 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u48RpneyLxDO"
   },
   "source": [
    "Let us test our first sentiment using VADER now.\n",
    "\n",
    "The Positive, Negative and Neutral scores represent the proportion of text that falls in these categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TZF4_fpDbLNB"
   },
   "source": [
    "#NLTK --> Vader\n",
    "(SentimentIntensityAnalyzer) results with a Machine Learning trained classifier for prediction of Sentiments on reddit data.\n",
    "VADER Sentiment Analysis :\n",
    "(Valence Aware Dictionary and sentiment Reasoner) NLTK built-in Vader sentiment analyzer Words that use positive and negative vocabulary will rank a piece of text as positive, negative or neutral. We can create one by first Emotional Strength Analyzer (SIA)To classify our title to take advantage of this tool, then we will use that polarity_scores Ways to get emotions.\n",
    "Acctually,\n",
    "Vader is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\n",
    "VADER uses a combination of A sentiment lexicon is a list of lexical features (e.g., words) which are generally labeled according to their semantic orientation as either positive, negative or neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "YHJftXom4MZa",
    "outputId": "36d3cbe5-6692-4bcb-b03c-019d1b161c37"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "SIA = SentimentIntensityAnalyzer()\n",
    "\n",
    "# the sentence and label column\n",
    "data_sents = list()\n",
    "data_class = list()\n",
    "\n",
    "# the label of each sentence\n",
    "n, o, p = (None, None, None)\n",
    "\n",
    "for comment in dataset:\n",
    "    if comment == \"[deleted]\":\n",
    "        continue\n",
    "    else:\n",
    "        \n",
    "        # polarity_scores method of SentimentIntensityAnalyzer \n",
    "        # oject gives a sentiment dictionary. \n",
    "        # which contains pos, neg, neu, and compound scores\n",
    "        data_sents.append(comment)\n",
    "        pol = SIA.polarity_scores(comment)\n",
    "        n = pol['neg']\n",
    "        o = pol['neu']\n",
    "        p = pol['pos']\n",
    "        \n",
    "        # decide sentiment as positive, negative and neutral\n",
    "        class_ = max(n,o,p)\n",
    "        if class_ == n:\n",
    "            data_class.append(-1)\n",
    "        elif class_ == o:\n",
    "            data_class.append(0)\n",
    "        elif class_ == p:\n",
    "            data_class.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "d99akeU24Mjz",
    "outputId": "2c8d9b20-e7b8-4669-e2b1-819f43a4faf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "neg = 0\n",
    "neu = 0\n",
    "pos = 0\n",
    "for i in data_class:\n",
    "  if  i == -1:\n",
    "    neg += 1\n",
    "  elif i == 0:\n",
    "    neu += 1\n",
    "  elif i == 1:\n",
    "    pos += 1\n",
    "\n",
    "plt.bar([-1, 0, 1], [neg, neu, pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "nrMmJXtghuNJ",
    "outputId": "c9bde193-1fda-4920-9b52-a24eccebd4c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative :  69103 \n",
      "Neutral  :  2720394 \n",
      "Positive :  94599\n"
     ]
    }
   ],
   "source": [
    "print(\"Negative : \", neg, \"\\nNeutral  : \", neu, \"\\nPositive : \", pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UYlCz_OpMGY0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame(data_sents) # columns=[\"body\"]\n",
    "df2 = pd.DataFrame(data_class) # columns=[\"sentiment\"]\n",
    "df = pd.concat([df1,df2] , axis=1 , sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "colab_type": "code",
    "id": "oNAOwE7TMrLs",
    "outputId": "2051e3a8-3f1f-4e14-f7a4-e43cd882b0dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good rant stop look for a mass movement if one...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sound good to me.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ok so peopl can donat chariti but how doe peop...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>delet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>whi so red.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884091</th>\n",
       "      <td>whi not includ the flight cost too.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884092</th>\n",
       "      <td>how be that humanli possible.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884093</th>\n",
       "      <td>I wouldn t be surpris if the xbla psn wii virt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884094</th>\n",
       "      <td>pidgeon.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884095</th>\n",
       "      <td>delet</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2884096 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         0  0\n",
       "0        good rant stop look for a mass movement if one...  0\n",
       "1                                        sound good to me.  0\n",
       "2        Ok so peopl can donat chariti but how doe peop...  0\n",
       "3                                                    delet  0\n",
       "4                                              whi so red.  0\n",
       "...                                                    ... ..\n",
       "2884091                whi not includ the flight cost too.  0\n",
       "2884092                      how be that humanli possible.  0\n",
       "2884093  I wouldn t be surpris if the xbla psn wii virt...  0\n",
       "2884094                                           pidgeon.  0\n",
       "2884095                                              delet  0\n",
       "\n",
       "[2884096 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ShBkTQNgbk7A"
   },
   "source": [
    "#A Confusion Matrix\n",
    "A confusion matrix can help understand the performance of the models.\n",
    "We done The metrics and the confusion matrix for each model (SVM, the other model, and baseline), to compare their performance more appropriately;\n",
    "Evaluation: To evaluate the model’s accuracy, a confusion matrix of the model is plotted using scikit-learn\n",
    "The confusion matrix tabulates the number of correct predictions versus the number of incorrect predictions for each class, so it becomes easier to see which classes are the least accurately predicted for a given classifier\n",
    "Transforming words to features: To transform the text into features, the first step is to use scikit-learn’s CountVectorizer.\n",
    "This converts the entire corpus (i.e. all sentences) of our training data into a matrix of token counts.\n",
    "Tokens (words, punctuation symbols, etc.) are created using NLTK’s tokenizer and commonly-used stop words like “a”, “an”, “the” are removed, because they do not add much value to the sentiment scoring.\n",
    "in the Next, the count matrix is converted to a TF-IDF (Term-frequency Inverse document frequency) representation. From the scikit-learn documentation #Confusion Matrix is used to understand the trained classifier behavior over the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "colab_type": "code",
    "id": "TsNqAXQf7fZO",
    "outputId": "0460683c-2c88-43de-ad96-1f561b9ed536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy:\n",
      "\n",
      "0.9438746533578263\n",
      "************************************************************************************\n",
      "\n",
      "confusion_matrix: \n",
      "\n",
      "[[  1212  21757     38]\n",
      " [   600 904203   1945]\n",
      " [   102  29515   1994]]\n",
      "************************************************************************************\n",
      "\n",
      "classification_report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.63      0.05      0.10     23007\n",
      "           0       0.95      1.00      0.97    906748\n",
      "           1       0.50      0.06      0.11     31611\n",
      "\n",
      "    accuracy                           0.94    961366\n",
      "   macro avg       0.69      0.37      0.39    961366\n",
      "weighted avg       0.92      0.94      0.92    961366\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer,CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(data_sents,data_class,\n",
    "                                                 test_size=float(1/3),random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# training with using gini index for decision tree\n",
    "train_gini=DecisionTreeClassifier(criterion='gini',max_depth=2)\n",
    "\n",
    "# i use pipelining because of that atleast there is atleast a word \n",
    "# in test data that not exist in training data\n",
    "gini_index=Pipeline([('vect_gini',CountVectorizer(stop_words=stop_words)),('tdidf_gini',\n",
    "                    TfidfTransformer()),('clfg',train_gini)])\n",
    "\n",
    "gini_index.fit(X_train,Y_train)\n",
    "test_pred_gini=gini_index.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"\\naccuracy:\\n\")\n",
    "print(accuracy_score(Y_test,test_pred_gini))\n",
    "#Confusion Matrix is used to understand the trained classifier behavior over the test dataset or validate dataset.\n",
    "print(\"************************************************************************************\")\n",
    "print(\"\\nconfusion_matrix: \\n\")\n",
    "print(confusion_matrix(Y_test,test_pred_gini))\n",
    "print(\"************************************************************************************\")\n",
    "print(\"\\nclassification_report:\\n\")\n",
    "print(classification_report(Y_test,test_pred_gini))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7e-83b6Sbrxd"
   },
   "source": [
    "Decision Tree Algorithm\n",
    "Once the data has been divided into the training and testing sets,\n",
    "the next step is to train the decision tree algorithm on this data and make predictions. we will use the DecisionTreeClassifier.\n",
    "The fit method of this class is called to train the algorithm on the training data, which is passed as parameter to the fit method.\n",
    "Evaluating the Algorithm\n",
    "At this point we have trained our algorithm and made some predictions. Now we'll see how accurate our algorithm is. For classification tasks some commonly used metrics are confusion matrix, precision, recall, and F1 score.\n",
    "Scikit-Learn's metrics library contains the classification_report and confusion_matrix methods that can be used to calculate these metrics for us: #training with using entropy (information gain) for decision tree What is Entropy? In the most layman terms, Entropy is nothing but the measure of disorder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "colab_type": "code",
    "id": "c0_dS8Bb7tR9",
    "outputId": "ba637603-8296-49b0-a615-4a865c50253a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy:\n",
      "\n",
      "0.943187090036469\n",
      "************************************************************************************\n",
      "\n",
      "confusion_matrix: \n",
      "\n",
      "[[     0  23007      0]\n",
      " [     0 906748      0]\n",
      " [     0  31611      0]]\n",
      "************************************************************************************\n",
      "\n",
      "classification_report:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00     23007\n",
      "           0       0.94      1.00      0.97    906748\n",
      "           1       0.00      0.00      0.00     31611\n",
      "\n",
      "    accuracy                           0.94    961366\n",
      "   macro avg       0.31      0.33      0.32    961366\n",
      "weighted avg       0.89      0.94      0.92    961366\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training with using entropy (information gain) for decision tree\n",
    "\n",
    "train_entropy=DecisionTreeClassifier(criterion='entropy',max_depth=2)\n",
    "entropy=Pipeline([('vect_entropy',TfidfVectorizer(stop_words=stop_words)),('tdidf_entropy',\n",
    "                 TfidfTransformer()),('clfe',train_entropy)])\n",
    "\n",
    "\n",
    "entropy.fit(X_train,Y_train)\n",
    "test_pred_entropy=entropy.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"\\naccuracy:\\n\")\n",
    "print(accuracy_score(Y_test,test_pred_entropy))\n",
    "print(\"************************************************************************************\")\n",
    "print(\"\\nconfusion_matrix: \\n\")\n",
    "print(confusion_matrix(Y_test,test_pred_entropy))\n",
    "print(\"************************************************************************************\")\n",
    "print(\"\\nclassification_report:\\n\")\n",
    "print(classification_report(Y_test,test_pred_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qh953nkdbx6I"
   },
   "source": [
    "Dummy Classification\n",
    "The dummy classifier gives you a measure of “baseline” performance — i.e. the success rate one should expect to achieve even if simply guessing.\n",
    "This classifier is useful as a simple baseline to compare with other (real) classifiers.\n",
    "A dummy classifier is a type of classifier which does not generate any insight about the data and classifies the given data using only simple rules\n",
    "It is used only as a simple baseline for the other classifiers\n",
    "i.e. any other classifier is expected to perform better on the given dataset. It is especially useful for datasets where are sure of a class imbalance.\n",
    "Actually, dummy classifier completely ignores the input data. In case of 'most frequent' method, it checks the occurrence of most frequent label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "1Ebit-Ic7tao",
    "outputId": "c8ce0f35-62a6-43ac-c4a8-5aa7266e3813"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/dummy.py:132: FutureWarning: The default value of strategy will change from stratified to prior in 0.24.\n",
      "  \"stratified to prior in 0.24.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy:\n",
      "\n",
      "0.943187090036469\n",
      "************************************************************************************\n",
      "\n",
      "confusion_matrix: \n",
      "\n",
      "[[     0  23007      0]\n",
      " [     0 906748      0]\n",
      " [     0  31611      0]]\n",
      "************************************************************************************\n",
      "\n",
      "classification_report:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00     23007\n",
      "           0       0.94      1.00      0.97    906748\n",
      "           1       0.00      0.00      0.00     31611\n",
      "\n",
      "    accuracy                           0.94    961366\n",
      "   macro avg       0.31      0.33      0.32    961366\n",
      "weighted avg       0.89      0.94      0.92    961366\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "train_dummy = DummyClassifier()\n",
    "model_dummy = Pipeline([('vect_dummy',TfidfVectorizer(stop_words=stop_words)),('tdidf_dummy',\n",
    "                 TfidfTransformer()),('dummy',train_dummy)])\n",
    "\n",
    "model_dummy.fit(X_train,Y_train)\n",
    "test_pred_dummy=entropy.predict(X_test)\n",
    "\n",
    "print(\"\\naccuracy:\\n\")\n",
    "print(accuracy_score(Y_test,test_pred_dummy))\n",
    "print(\"************************************************************************************\")\n",
    "print(\"\\nconfusion_matrix: \\n\")\n",
    "print(confusion_matrix(Y_test,test_pred_dummy))\n",
    "print(\"************************************************************************************\")\n",
    "print(\"\\nclassification_report:\\n\")\n",
    "print(classification_report(Y_test,test_pred_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ulJVbM9Qb5xp"
   },
   "source": [
    "LinearSVC\n",
    "Now we used in this project is a support vector machine,\n",
    "This model created with using the Sklearn Library,\n",
    "And will be taught with the training data created in the following steps.\n",
    "In the figure , you can see the code for creating, Training , and evaluating the model.\n",
    "Here, 25% of the training data is considered as evaluation data for the selection of hyperparameters.\n",
    "The value of c, which is the input of the backup vector machine model, is considered to be 0.5 , Evaluate for best performance on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "3xuK-7hK7yz8",
    "outputId": "c868e713-b69a-4255-9704-47e5f6e4e5ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy:\n",
      "\n",
      "0.9673100567317754\n",
      "************************************************************************************\n",
      "\n",
      "confusion_matrix: \n",
      "\n",
      "[[  9759  12867    381]\n",
      " [  2152 901100   3496]\n",
      " [   340  12191  19080]]\n",
      "************************************************************************************\n",
      "\n",
      "classification_report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.42      0.55     23007\n",
      "           0       0.97      0.99      0.98    906748\n",
      "           1       0.83      0.60      0.70     31611\n",
      "\n",
      "    accuracy                           0.97    961366\n",
      "   macro avg       0.87      0.67      0.75    961366\n",
      "weighted avg       0.96      0.97      0.96    961366\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "train_LinearSVC = LinearSVC(C=0.5)\n",
    "model_LinearSVC = Pipeline([('vect_LinearSVC',TfidfVectorizer(stop_words=stop_words)),('tdidf_LinearSVC',\n",
    "                 TfidfTransformer()),('LinearSVC',train_LinearSVC)])\n",
    "\n",
    "model_LinearSVC.fit(X_train,Y_train)\n",
    "test_pred_LinearSVC=model_LinearSVC.predict(X_test)\n",
    "\n",
    "print(\"\\naccuracy:\\n\")\n",
    "print(accuracy_score(Y_test,test_pred_LinearSVC))\n",
    "print(\"************************************************************************************\")\n",
    "print(\"\\nconfusion_matrix: \\n\")\n",
    "print(confusion_matrix(Y_test,test_pred_LinearSVC))\n",
    "print(\"************************************************************************************\")\n",
    "print(\"\\nclassification_report:\\n\")\n",
    "print(classification_report(Y_test,test_pred_LinearSVC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jcxC6YVAbZVl"
   },
   "source": [
    "characteristics of a Positive, Negative And Neutral comment,\n",
    "sentiment analysis or sentiment classification fall into the broad category of text classification tasks where you are supplied with a phrase, or a list of phrases and your classifier is supposed to tell if the sentiment behind that is positive, negative or neutral.\n",
    "Consider the following phrases:\n",
    "1. \"i love this movie.\"\n",
    "2. \"this movie is not a great movie.\"\n",
    "3. \"This is a movie.\"\n",
    "The phrases correspond to short movie reviews, and each one of them conveys different sentiments.\n",
    "For example,\n",
    "the first phrase denotes positive sentiment about the film,\n",
    "while the second one treats the movie as not so great (negative sentiment).\n",
    "Take a look at the third one more closely. There is no such word in that phrase which can tell you about anything regarding the sentiment conveyed by it. Hence, that is an example of neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CnvOBsK-72uc"
   },
   "outputs": [],
   "source": [
    "pol = dict()\n",
    "pol['neg'] = list()\n",
    "pol['neu'] = list()\n",
    "pol['pos'] = list()\n",
    "\n",
    "for i in range(int(len(X_test)/10)):\n",
    "    if test_pred_gini[i] == -1:\n",
    "        pol['neg'].append(i)\n",
    "    elif test_pred_gini[i] == 0:\n",
    "        pol['neu'].append(i)\n",
    "    elif test_pred_gini[i] == 1:\n",
    "        pol['pos'].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "MlV33UH07259",
    "outputId": "61244571-6f21-4454-caf3-375f94fa4bd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Comments\n",
      "************************************************************************\n",
      "To the ministri of love with you\n",
      "\n",
      "\n",
      "love how terror be call defens when a countri doe it.\n",
      "\n",
      "\n",
      "Oh but they do. and they love it.\n",
      "\n",
      "\n",
      "there be even bird and shit repres my love to you.\n",
      "\n",
      "\n",
      "love zach galifianakis. love whoever make thi beat. hate kany west.\n",
      "\n",
      "\n",
      "I think I m the onli one who love the mako C\n",
      "\n",
      "\n",
      "I love thi movie.\n",
      "\n",
      "\n",
      "zionist will love you for that.\n",
      "\n",
      "\n",
      "I love that yahtze say spacker .\n",
      "\n",
      "\n",
      "god I love futurama.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some Positive Comments\n",
    "print(\"Positive Comments\")\n",
    "print(\"************************************************************************\")\n",
    "for i in range(10):\n",
    "    print(X_test[pol['pos'].pop()])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "id": "l-I3HJ3Z72_i",
    "outputId": "08d76bc2-19ad-4b2b-abac-859ad68934f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Comments\n",
      "************************************************************************\n",
      "what the fuck do they alway point at.\n",
      "\n",
      "\n",
      "fuck yeah.\n",
      "\n",
      "\n",
      "I love it.funni as fuck.\n",
      "\n",
      "\n",
      "I don t know what the fuck you just say but I think I like it.\n",
      "\n",
      "\n",
      "do you fuck her.\n",
      "\n",
      "\n",
      "fuck yeah. best ever.\n",
      "\n",
      "\n",
      "fuck moron.\n",
      "\n",
      "\n",
      "Oh fuck my father doe this. It irrit the fuck out of me. serious I come over to see you and all you can do be fuck around on your phone. and he wonder whi I don t go see him veri often.\n",
      "\n",
      "\n",
      "the fuck.\n",
      "\n",
      "\n",
      "fuck no.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some Negative Comments\n",
    "print(\"Negative Comments\")\n",
    "print(\"************************************************************************\")\n",
    "for i in range(10):\n",
    "    print(X_test[pol['neg'].pop()])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "id": "_YlPMnDY73Cp",
    "outputId": "067aea1a-e9d3-450e-fe1f-9b479ea85996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral Comments\n",
      "************************************************************************\n",
      "for rizzle.\n",
      "\n",
      "\n",
      "It be dark.\n",
      "\n",
      "\n",
      "clownin http imgur.com xsgb.jpg\n",
      "\n",
      "\n",
      "rodrigo y gabriela s version of stairway to heaven\n",
      "\n",
      "\n",
      "whi isn t fox shut down or fine everi time they have an idiot guest pundit say someth irrat and illog they report someth fals or misrepres everyth to cater to redneck s preconceiv notions. what better way to repay our nation debt.\n",
      "\n",
      "\n",
      "she s kinda hot. for a midget.\n",
      "\n",
      "\n",
      "gt; take away tip and you take away ani incent I have to get there quickly. If the owner of the pizza place give you a decent paycheck from the start wouldn t thi be an incent to deliv more efficiently. plu you d have an incent of not get complaint from the custom and eventu lose your job right. .\n",
      "\n",
      "\n",
      "gt;whi do you need to believ a god exist in the first place. I don t. I don t need to believ anything. I onli need to be cogniz of who I be or what it mean to be me. and an essenti part of what it mean to be me be the knowledg that I experi intern conflicts. one pole of thi conflict I can name the ego or the small me . the other pole of thi conflict I can name the self or even god . john again the jew pick up stone to stone him but jesu say to them I have show you mani great miracl from the father. for which of these do you stone me. We be not stone you for ani of these repli the jew but for blasphemi becaus you a mere man claim to be god. jesu answer them Is it not write in your law I have say you be god . psalm I say you be god ; you be all son of the most high.\n",
      "\n",
      "\n",
      "I ll help you start. . It wa a dark and stormi night.\n",
      "\n",
      "\n",
      "dear lord he know pornstar too like the back of hi hand. althought I don t know whi he ask if te pornstar wa innov or know fairi .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some Neutral Comments\n",
    "print(\"Neutral Comments\")\n",
    "print(\"************************************************************************\")\n",
    "for i in range(10):\n",
    "    print(X_test[pol['neu'].pop()])\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
